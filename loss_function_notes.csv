Loss function,Papers,Original Application,Short explanation,network used in paper(s),Advantages,Disadvantages,Comments &Remarks,Codebase name,best learning rate
Riemannian Loss,https://openaccess.thecvf.com/content_CVPRW_2019/papers/WiCV/Mu_Riemannian_Loss_for_Image_Restoration_CVPRW_2019_paper.pdf,Image restoration (general),A loss function that measures geodistic distance in Riemannian manifold; the loss in Riemannian reflects the structure distance of the image in question.,,,Tends to over-smooth inputs?,,RiemannianLoss(gamma=),1e-3
MSE/L2,https://arxiv.org/pdf/1511.08861,Image restoration (general),Pixel-level loss that sums the squared differences between ground truth and predicted images,,"Easy to use and implement, fast to train, common",Tends to over-smooth outputs,,torch.nn.MSELoss(),1e-2
L1/MAE,https://arxiv.org/pdf/1511.08861,Image restoration (general),A similar function to MSE (L2) which sums abs. value of difference between GT and predictions,,"Easy to use and implement, fast to train, common","With Unet, it seems to result in images that look like an insect screen was overlaid on it",,torch.nn.L1Loss(),1e-2
Huber Loss,"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923297, and https://openaccess.thecvf.com/content/CVPR2021/papers/Meyer_An_Alternative_Probabilistic_Interpretation_of_the_Huber_Loss_CVPR_2021_paper.pdf",Signal extraction,"Huber loss ""combines advantages of both L1Loss and MSELoss; the delta-scaled L1 region makes the loss less sensitive to outliers than MSELoss, while the L2 region provides smoothness over L1Loss near 0"".","R-CNN, RetinaNet",Customizeable (gamma parameter allows you to alter L1 vs L2),Tends to bias in favor of lighter colors in greyscale noisy images at gamma=0.75,Try with many different gamma parameters?,"torch.nn.HuberLoss(reduction=,delta=)",1e-3
Edge+MAE Loss,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461664,Image restoration (general),"Makes use of edgemaps: sums edge loss and pixel loss to create an overall loss, combined with MAE",,"Customizeable alpha parameter (called it sigma in the code, remember to change that sometime)","Long training with higher costs (possibly further optimizable), not available as a Pytorch built-in",Curious about how to most efficiently generate the edgemaps needed for this func.,EdgeLoss(alpha=),1e-3
PSNR,https://ieeexplore.ieee.org/abstract/document/5596999,Image restoration (general),,,Low PSNR,VERY high MSE compared to others,Had to add a multiply loss by -1 to the original function to make the results more normal,PSNR/PSNR_Loss,1e-3
Feature Preserving Loss,2310.20101 (arxiv.org),medical image restoration,FPL makes the neural network less concerned on global smoothing and moreso on feature weights; MSE and residual learnings aren't designed to distinguish between features and noise within the same pixel,U-net,Specifically designed to distinguish features and noise in medical imaging,Relatively low SSIM,,FeaturePreservingLoss(),1e-3
Generalized Loss (no specific name given in paper),1701.03077 (arxiv.org),Medical image restoration,"Generalization of several different loss functions (eg Cauchy/Lorentzian, L1, Huber, Charbonnier, etc)",,Highly customizeable alpha and c parameters for different applications,Highly customizeable alpha and c parameters for different applications,"Experimenting with different alpha and c values, a=1.25 and c=2 is an ok baseline, more experimentation needed","GeneralizedLoss(alpha=,c=)",1e-3
Charbonnier Loss,CT-Scan Denoising Using a Charbonnier Loss Generative Adversarial Network | IEEE Journals & Magazine | IEEE Xplore,CT-scan noise reduction,,VGG-19,,,,Charbonnier_Loss(),1e-3
Perceptual Loss,[1603.08155] Perceptual Losses for Real-Time Style Transfer and Super-Resolution (arxiv.org),,,,,,,,
SSIM,https://research.nvidia.com/sites/default/files/pubs/2017-03_Loss-Functions-for/NN_ImgProc.pdf,,,,,,,,
MS-SSIM,https://research.nvidia.com/sites/default/files/pubs/2017-03_Loss-Functions-for/NN_ImgProc.pdf,,,,,,,,
Contextual Loss,https://arxiv.org/abs/1803.02077,,,,,,,,
Frequency domain loss,https://arxiv.org/pdf/2104.10856,,,,,,,,
Frequency domain perceptual loss,https://arxiv.org/abs/2007.12296,,,,,,,,
Focal Frequency loss,https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Focal_Frequency_Loss_for_Image_Reconstruction_and_Synthesis_ICCV_2021_paper.pdf,,,,,,Implementation: https://github.com/EndlessSora/focal-frequency-loss,,
Topological loss,https://www.sciencedirect.com/science/article/pii/S016516842300155X,,,,,,,,
Total variation loss,,,,,,,Documentation: https://lightning.ai/docs/torchmetrics/stable/image/total_variation.html,,